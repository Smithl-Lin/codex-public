import os
import json
import time
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from google.api_core.exceptions import ResourceExhausted, InternalServerError, ServiceUnavailable
from tqdm import tqdm

# ================= é…ç½®åŒº =================
try:
    from config import get_gemini_api_key
    API_KEY = get_gemini_api_key()
except Exception:
    API_KEY = None
INPUT_DIR = "AMANI_Training_Data"
OUTPUT_FILE = "amani_finetuning_dataset.jsonl"

if API_KEY:
    genai.configure(api_key=API_KEY)

# ğŸ”„ æˆ˜ç•¥åˆ‡æ¢ï¼šä½¿ç”¨ Lite é¢„è§ˆç‰ˆï¼Œäº«æœ‰ç‹¬ç«‹é…é¢æ± 
MODEL_NAME = "models/gemini-2.0-flash-lite-preview-02-05"

# ğŸ›¡ï¸ åŒ»ç–—å®‰å…¨è±å… (ä¿æŒä¸æ‹¦æˆª)
SAFETY_SETTINGS = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
}

SYSTEM_PROMPT = """
You are A.M.A.N.I., an expert clinical resource coordinator.
Analyze the provided medical case abstract. Extract key information into the A.M.A.N.I. 4-Level Ontology.

OUTPUT SCHEMA (Strict JSON):
{
  "L1_Anchor": { "Diagnosis": "...", "Contraindications": "...", "Weight": 1.0 },
  "L2_Clinical": { "Symptoms": "...", "History": "...", "Trial_Criteria": "...", "Weight": 0.8 },
  "L3_Profile": { "Demographics": "...", "Financial_Proxy": "...", "Weight": 0.6 },
  "L4_Context": { "Preferences": "...", "Weight": 0.3 }
}
RULES: Output ONLY valid JSON. No markdown.
"""

def clean_json_string(json_str):
    json_str = json_str.strip()
    if json_str.startswith("```json"): json_str = json_str[7:]
    if json_str.startswith("```"): json_str = json_str[3:]
    if json_str.endswith("```"): json_str = json_str[:-3]
    return json_str.strip()

def process_case_with_retry(model, case_data):
    """å¸¦è‡ªåŠ¨é‡è¯•æœºåˆ¶çš„å¤„ç†å‡½æ•°"""
    abstract = case_data.get('abstract', '')
    if len(abstract) < 50: return None

    full_prompt = f"{SYSTEM_PROMPT}\n\nCASE ABSTRACT:\n{abstract}"
    
    max_retries = 5
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            # å‘é€è¯·æ±‚
            response = model.generate_content(
                full_prompt, 
                safety_settings=SAFETY_SETTINGS
            )
            
            cleaned_response = clean_json_string(response.text)
            structured_logic = json.loads(cleaned_response)
            
            return {
                "instruction": "Analyze this medical case and extract the A.M.A.N.I. 4-Level Ontology.",
                "input": abstract,
                "output": json.dumps(structured_logic, ensure_ascii=False)
            }

        except ResourceExhausted:
            # ğŸ›‘ é‡åˆ° 429 è¶…é€Ÿï¼Œè‡ªåŠ¨åœè½¦ç­‰å¾…
            # è¿™é‡Œçš„ç­‰å¾…æ—¶é—´è¾ƒçŸ­ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»æ¢äº† Lite æ¨¡å‹
            wait_time = 30 + (retry_count * 10)
            tqdm.write(f"â³ Quota Hit (429). Sleeping for {wait_time}s to cooldown...")
            time.sleep(wait_time)
            retry_count += 1
            
        except (InternalServerError, ServiceUnavailable):
            time.sleep(5)
            retry_count += 1
            
        except Exception as e:
            # å…¶ä»–é”™è¯¯ç›´æ¥è·³è¿‡
            return None
            
    return None

def main():
    print(f"=== A.M.A.N.I. Data Synthesizer V3.1 (Lite Edition) ===")
    print(f"ğŸ¤– Model: {MODEL_NAME}")
    print(f"ğŸ¢ Speed Limit: 10s delay (Maximum Safety Mode)")
    
    try:
        model = genai.GenerativeModel(MODEL_NAME)
    except Exception as e:
        print(f"âŒ Error initializing model: {e}")
        return

    if not os.path.exists(INPUT_DIR): return
    
    # 1. è¯»å–åŸå§‹æ•°æ®
    all_files = [f for f in os.listdir(INPUT_DIR) if f.endswith('_training_set.json')]
    all_cases = []
    for f in all_files:
        try:
            with open(os.path.join(INPUT_DIR, f), 'r', encoding='utf-8') as file:
                all_cases.extend(json.load(file))
        except: continue
    
    # 2. æ–­ç‚¹ç»­ä¼ é€»è¾‘
    existing_outputs = []
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    existing_outputs.append(json.loads(line))
                except: continue
    
    processed_count = len(existing_outputs)
    print(f"ğŸ“‚ Total Raw Cases: {len(all_cases)}")
    print(f"â™»ï¸  Resuming from: {processed_count} (Already done)")
    
    cases_to_process = all_cases[processed_count:]
    
    if not cases_to_process:
        print("âœ… All cases already processed!")
        return

    print(f"ğŸš€ Starting synthesis for remaining {len(cases_to_process)} cases...")

    successful_this_run = 0
    
    # è¿½åŠ æ¨¡å¼ 'a'
    with open(OUTPUT_FILE, 'a', encoding='utf-8') as outfile:
        for case in tqdm(cases_to_process): 
            result = process_case_with_retry(model, case)
            
            if result:
                outfile.write(json.dumps(result, ensure_ascii=False) + '\n')
                outfile.flush() 
                successful_this_run += 1
            
            # ğŸ›‘ å¼ºåˆ¶ 10 ç§’å»¶è¿Ÿ - è¿™æ˜¯æ ¸å¿ƒä¿®æ”¹
            time.sleep(10) 

    print("\n=============================================")
    print(f"âœ… Synthesis Complete.")
    print(f"ğŸ“Š New Added: {successful_this_run}")
    print(f"ğŸ’¾ Total Data: {processed_count + successful_this_run}")
    print("=============================================")

if __name__ == "__main__":
    main()