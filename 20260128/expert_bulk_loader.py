import chromadb
import json
import time
import numpy as np
from chromadb.utils import embedding_functions

class AMAHBulkExpertEngine:
    def __init__(self):
        # 1. åˆå§‹åŒ–æŒä¹…åŒ–å®¢æˆ·ç«¯
        self.client = chromadb.PersistentClient(path="./amah_vector_db")
        
        # 2. æ ¸å¿ƒç²¾åº¦å‚æ•°é…ç½®
        # è°ƒé«˜ M å’Œ construction_ef ä»¥ç¡®ä¿ 10ä¸‡çº§æ•°æ®ä¸‹çš„ 0.79 åŒ¹é…ç²¾åº¦
        self.collection = self.client.get_or_create_collection(
            name="expert_map_global",
            metadata={
                "hnsw:space": "cosine", 
                "hnsw:construction_ef": 400, # æ·±åº¦ç´¢å¼•æ„å»º
                "hnsw:M": 32                 # å¢åŠ è¿æ¥æ•°ï¼Œæå‡å¬å›ç‡
            }
        )
        print("ğŸ›ï¸ AMAH å…¨çƒä¸“å®¶ç´¢å¼•ç©ºé—´å·²ç¡¬åŒ–ã€‚")

    def batch_import(self, data_list, batch_size=100):
        """
        åˆ†æ‰¹å¯¼å…¥é€»è¾‘ï¼Œé˜²æ­¢å†…å­˜æº¢å‡ºï¼Œç¡®ä¿åç»­æŒç»­æ›´æ–°ã€‚
        """
        total = len(data_list)
        print(f"ğŸ“¦ å‡†å¤‡å¤„ç† {total} ä¸ªä¸“å®¶èŠ‚ç‚¹...")
        
        for i in range(0, total, batch_size):
            batch = data_list[i : i + batch_size]
            ids = [item['id'] for item in batch]
            # å¢å¼ºå‹è¯­ä¹‰æ–‡æœ¬ï¼šæ•´åˆå…¨ç”Ÿå‘½å‘¨æœŸæœåŠ¡æ ‡ç­¾
            documents = [
                f"{item['name']} | {item['affiliation']} | {item['specialty']} | "
                f"Tags: {', '.join(item['expertise_tags'])} | "
                f"Services: {', '.join(item['value_add_services'])}" 
                for item in batch
            ]
            metadatas = [{
                "name": item['name'],
                "location": f"{item['location']['city']}, {item['location']['state']}",
                "insurance": json.dumps(item['insurance_partners'])
            } for item in batch]

            self.collection.upsert(ids=ids, documents=documents, metadatas=metadatas)
            print(f"âœ… å·²å®Œæˆ: {min(i + batch_size, total)} / {total}")

    def unified_match_audit(self, query):
        """
        å…¨è·¯å¾„ç²¾å‡†åŒ¹é…éªŒè¯
        """
        start = time.time()
        results = self.collection.query(
            query_texts=[query],
            n_results=3,
            include=['documents', 'distances', 'metadatas']
        )
        
        print(f"\nğŸ“Š æ£€ç´¢è€—æ—¶: {time.time()-start:.4f}s")
        for i in range(len(results['ids'][0])):
            accuracy = 1 - results['distances'][0][i]
            if accuracy >= 0.79:
                print(f"ğŸ¯ [ç²¾å‡†å¯¹ä½] {results['metadatas'][0][i]['name']} | ç²¾åº¦: {accuracy:.4f}")
                print(f"ğŸ”— èµ„æºæŒ‡å‘: {results['documents'][0][i][:120]}...")
            else:
                print(f"âš ï¸ [ä½ç²¾æ‹¦æˆª] åŒ¹é…åº¦ {accuracy:.4f} æœªè¾¾ 0.79 é˜ˆå€¼ã€‚")

if __name__ == "__main__":
    engine = AMAHBulkExpertEngine()
    
    # æ¨¡æ‹ŸçœŸå® 100+ ä¸“å®¶å¯¼å…¥ (å®é™…å¯å¯¹æ¥æ‚¨çš„ CSV/æ•°æ®åº“)
    # è¿™é‡Œæˆ‘ä»¬å¤ç”¨ä¹‹å‰çš„æ•°æ®ï¼Œä½†åœ¨å®é™…è·¯æ¼”æ—¶ï¼Œå»ºè®®åŠ è½½æ‚¨å®Œæ•´çš„ä¸“å®¶æ¸…å•
    try:
        with open('expert_map_data.json', 'r') as f:
            data = json.load(f)
            engine.batch_import(data)
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆç”Ÿæˆ expert_map_data.json")

    # éªŒè¯åŒ¹é…å‡†ç¡®æ€§ä¸ç­–ç•¥å»¶ç»­æ€§
    test_query = "Find STN-DBS experts in Florida for refractory tremors, must handle international travel."
    engine.unified_match_audit(test_query)
