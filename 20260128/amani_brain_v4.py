# V4.0_STRATEGIC_LOCKED_BY_SMITH_LIN
# -*- coding: utf-8 -*-
# ==============================================================================
# ğŸ§  A.M.A.N.I. TRINITY ENGINE V4 â€” å“¨å…µå±‚ä¼˜åŒ–
# ==============================================================================
# å“¨å…µå±‚ä¼˜åŒ–ï¼šEntropyUtils å®æ—¶è¾“å‡ºå¼ºåˆ¶æ³¨å…¥ E-CNN ç¬¬äºŒé€šé“ï¼Œæ›¿æ¢æ¨¡æ‹Ÿæ‹¼æ¥ã€‚
# Channel 1 = è¯­ä¹‰åµŒå…¥ | Channel 2 = EntropyUtils.calculate_sliding_entropy å®æ—¶ç†µæ³¢å½¢
# ==============================================================================

import math
import collections
import hashlib
import numpy as np
import time

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
except ImportError:
    print("âŒ ä¸¥é‡é”™è¯¯: ç¼ºå°‘ PyTorchã€‚è¯·è¿è¡Œ: pip install torch torchvision")
    raise

# ------------------------------------------------------------------------------
# AGID ä½“ç³»
# ------------------------------------------------------------------------------
def to_agid(namespace: str, node_type: str, raw_id) -> str:
    sid = hashlib.sha256(f"{namespace}:{node_type}:{raw_id}".encode()).hexdigest()[:12].upper()
    return f"AGID-{namespace}-{node_type}-{sid}"


# ==============================================================================
# ğŸ§© EntropyUtils â€” å…¨æ¯ç†µçº¹ç†ï¼Œå®æ—¶è¾“å‡ºæ³¨å…¥ E-CNN ç¬¬äºŒé€šé“
# ==============================================================================
class EntropyUtils:
    @staticmethod
    def calculate_sliding_entropy(text, window_size=5):
        """
        è®¡ç®—æ–‡æœ¬ç†µçº¹ç†ï¼ˆæ³¢å½¢ï¼‰ã€‚å®æ—¶è¾“å‡ºï¼Œå¼ºåˆ¶ä½œä¸º E-CNN ç¬¬äºŒé€šé“è¾“å…¥ï¼Œç¦æ­¢æ¨¡æ‹Ÿæ‹¼æ¥ã€‚
        è¿”å› (tensor [1, seq, 1], variance)
        """
        tokens = list(text)
        seq_len = len(tokens)
        entropy_seq = []

        if seq_len == 0:
            return torch.zeros(1, 1, 1), 0.0

        for i in range(seq_len):
            start = max(0, i - window_size // 2)
            end = min(seq_len, i + window_size // 2 + 1)
            window = tokens[start:end]
            counts = collections.Counter(window)
            ent = 0.0
            total = len(window)
            for count in counts.values():
                p = count / total
                ent -= p * math.log2(p) if p > 0 else 0
            entropy_seq.append(ent)

        # å®æ—¶ç†µåºåˆ— â†’ [1, seq_len, 1]ï¼Œä½œä¸º E-CNN ç¬¬äºŒé€šé“å”¯ä¸€æ¥æº
        channel2_entropy = torch.tensor(entropy_seq, dtype=torch.float32).unsqueeze(0).unsqueeze(2)
        variance = np.var(entropy_seq) if entropy_seq else 0.0
        return channel2_entropy, variance

    @staticmethod
    def variance_physical_intercept(variance: float, threshold: float = 0.005) -> bool:
        return float(variance) > threshold


# ==============================================================================
# ğŸ›¡ï¸ E-CNN å“¨å…µ â€” ç¬¬äºŒé€šé“å¼ºåˆ¶ä¸º EntropyUtils å®æ—¶è¾“å‡ºï¼ˆæ— æ¨¡æ‹Ÿæ‹¼æ¥ï¼‰
# ==============================================================================
class ECNN_Sentinel(nn.Module):
    """
    Channel 1: è¯­ä¹‰åµŒå…¥ (embed_dim)
    Channel 2: ä»…æ¥å— EntropyUtils.calculate_sliding_entropy çš„å®æ—¶è¾“å‡ºï¼Œä¸æ¥æ¨¡æ‹Ÿå€¼ã€‚
    """

    def __init__(self, vocab_size=5000, embed_dim=128, num_filters=64, kernel_sizes=[3, 4, 5]):
        super(ECNN_Sentinel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        # in_channels = embed_dim + 1ï¼Œå…¶ä¸­ +1 ä¸ºç¬¬äºŒé€šé“ï¼ˆç†µï¼‰
        self.convs = nn.ModuleList([
            nn.Conv1d(in_channels=embed_dim + 1, out_channels=num_filters, kernel_size=k)
            for k in kernel_sizes
        ])
        self.dropout = nn.Dropout(0.5)
        self.fc = nn.Linear(len(kernel_sizes) * num_filters, 3)

    def forward(self, x_indices, channel2_entropy_tensor):
        """
        x_indices: [B, L]; channel2_entropy_tensor: [B, L, 1] æ¥è‡ª EntropyUtils å®æ—¶è¾“å‡ºã€‚
        ç¦æ­¢ä¼ å…¥æ¨¡æ‹Ÿæ‹¼æ¥çš„å ä½ tensorï¼Œå¿…é¡»ä¸º calculate_sliding_entropy çš„è¿”å›å€¼ã€‚
        """
        # Channel 1: è¯­ä¹‰
        ch1_embed = self.embedding(x_indices)  # [B, L, embed_dim]
        # Channel 2: å¼ºåˆ¶ä¸º EntropyUtils å®æ—¶ç†µï¼Œæ›¿æ¢åŸæ¨¡æ‹Ÿæ‹¼æ¥
        combined = torch.cat((ch1_embed, channel2_entropy_tensor), dim=2)  # [B, L, embed_dim+1]
        combined = combined.permute(0, 2, 1)  # [B, embed_dim+1, L]
        conved = [F.relu(conv(combined)) for conv in self.convs]
        pooled = [F.adaptive_max_pool1d(conv, 1).squeeze(2) for conv in conved]
        cat = torch.cat(pooled, dim=1)
        cat = self.dropout(cat)
        logits = self.fc(cat)
        return F.softmax(logits, dim=1)


# ==============================================================================
# ğŸ”— GNN Nexus â€” è¾“å‡º AGID
# ==============================================================================
class GNN_Nexus_Sim(nn.Module):
    def __init__(self, num_assets=200000, feature_dim=192):
        super(GNN_Nexus_Sim, self).__init__()
        self.asset_memory = nn.Parameter(torch.randn(100, feature_dim))
        self.query_proj = nn.Linear(192, feature_dim)

    def forward(self, intent_vector):
        query = self.query_proj(intent_vector)
        scores = torch.matmul(query, self.asset_memory.t())
        attention = F.softmax(scores, dim=1)
        best_asset_idx = torch.argmax(attention, dim=1)
        return best_asset_idx, attention


# ==============================================================================
# ğŸ§  AMANI Brain V4 â€” å“¨å…µå±‚ä¼˜åŒ–ï¼šä»…ç”¨ EntropyUtils å®æ—¶è¾“å‡ºæ³¨å…¥ E-CNN
# ==============================================================================
class AMANI_Brain:
    def __init__(self):
        print("ğŸ§  æ­£åœ¨åˆå§‹åŒ– A.M.A.N.I. Trinity Engine V4 (å“¨å…µå±‚ä¼˜åŒ–ï¼šç†µå®æ—¶æ³¨å…¥ E-CNN ç¬¬äºŒé€šé“)...")
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.sentinel = ECNN_Sentinel().to(self.device)
        self.nexus = GNN_Nexus_Sim().to(self.device)
        self.sentinel.eval()
        self.VARIANCE_INTERCEPT_THRESHOLD = 0.005

    def _text_to_tensor(self, text):
        indices = [hash(c) % 5000 for c in text]
        return torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(self.device)

    def process_request(self, text):
        print(f"\nğŸ“© [INPUT] \"{text}\"")
        start_time = time.time()

        # 1. EntropyUtils å®æ—¶è¾“å‡ºï¼Œä½œä¸º E-CNN ç¬¬äºŒé€šé“å”¯ä¸€è¾“å…¥ï¼ˆæ— æ¨¡æ‹Ÿæ‹¼æ¥ï¼‰
        channel2_entropy, entropy_variance = EntropyUtils.calculate_sliding_entropy(text)
        channel2_entropy = channel2_entropy.to(self.device)
        avg_entropy = channel2_entropy.mean().item()
        print(f"   ğŸŒŠ å…¨æ¯ç†µ(å®æ—¶â†’E-CNN ç¬¬äºŒé€šé“): å‡å€¼ {avg_entropy:.4f} | æ–¹å·® {entropy_variance:.6f}")

        # 2. variance > 0.005 ç‰©ç†æ‹¦æˆª
        if EntropyUtils.variance_physical_intercept(entropy_variance, self.VARIANCE_INTERCEPT_THRESHOLD):
            agid_intercept = to_agid("MAYO", "INTERCEPT", f"var_{entropy_variance:.6f}")
            print(f"   ğŸš« ç‰©ç†æ‹¦æˆª: æ–¹å·® {entropy_variance:.6f} > 0.005 | {agid_intercept}")
            return agid_intercept

        # 3. å“¨å…µï¼šç¬¬äºŒé€šé“å¼ºåˆ¶ä¸º EntropyUtils å®æ—¶è¾“å‡º
        x_indices = self._text_to_tensor(text)
        with torch.no_grad():
            routing_weights = self.sentinel(x_indices, channel2_entropy)

        w_ethnic, w_geo, w_lang = routing_weights[0].tolist()
        print(f"   ğŸ§  è·¯ç”±: [æ—ç¾¤]{w_ethnic:.2f} [åŒºåŸŸ]{w_geo:.2f} [è¯­è¨€]{w_lang:.2f}")

        if avg_entropy < 1.5 or w_geo > 0.5:
            dummy_intent = torch.randn(1, 192).to(self.device)
            asset_id, _ = self.nexus(dummy_intent)
            agid_node = to_agid("MAYO", "NODE", 200000 + asset_id.item())
            print(f"   ğŸ“ GNN é”å®š: {agid_node}")
            final_decision = agid_node
        else:
            final_decision = to_agid("MAYO", "MODE", "MIXED_EMPATHY")

        print(f"   âœ… è¾“å‡ºèŠ‚ç‚¹: {final_decision} | â±ï¸ {(time.time()-start_time)*1000:.2f} ms")
        return final_decision


if __name__ == "__main__":
    brain = AMANI_Brain()
    brain.process_request("åŒ»ç”Ÿï¼Œæˆ‘è§‰å¾—æœ€è¿‘æœ‰ç‚¹å¿ƒæ…Œã€‚")
    brain.process_request("å‡†å¤‡é˜‘å°¾åˆ‡é™¤æœ¯ï¼Œé™è„‰æ³¨å°„ä¸™æ³Šé…š20mgã€‚")
