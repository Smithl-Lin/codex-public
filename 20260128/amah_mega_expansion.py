import chromadb
import json
import random
import time

class AMAHScaleEngine:
    def __init__(self):
        self.client = chromadb.PersistentClient(path="./amah_vector_db")
        self.collection = self.client.get_collection("expert_map_global")
        
    def generate_clinical_fingerprint(self, i):
        """ç”Ÿæˆå…·å¤‡æˆ˜ç•¥å»¶ç»­æ€§çš„ä¸´åºŠæŒ‡çº¹æ•°æ®"""
        hubs = [
            ("Jacksonville", "FL", "Mayo Clinic"),
            ("Houston", "TX", "Houston Methodist"),
            ("Rochester", "MN", "Mayo Clinic"),
            ("Cleveland", "OH", "Cleveland Clinic"),
            ("Boston", "MA", "MGH"),
            ("Palo Alto", "CA", "Stanford")
        ]
        city, state, aff = random.choice(hubs)
        
        # æ¨¡æ‹Ÿæœ€æ–°åŒ»ç–—å™¨æ¢°ä¸è¯ç‰©çš„å…³è”
        techs = ["STN-DBS", "GPi-DBS", "Focused-Ultrasound", "Gene-Therapy", "Monoclonal-Antibodies"]
        trials = ["Phase-III-Enrolled", "FDA-Breakthrough-Device", "Latest-Clinical-Protocol"]
        
        selected_tech = random.choice(techs)
        
        return {
            "id": f"mega_node_{i:06d}",
            "document": f"{aff} {city} {state} | {selected_tech} Expert | {random.choice(trials)} | Medicare BlueCross | Travel-Concierge Hospital-Docking",
            "metadata": {
                "name": f"Dr. {random.choice(['Smith', 'Lin', 'Garcia', 'Chen', 'Taylor'])}_{i}",
                "location": f"{city}, {state}",
                "services": json.dumps(["Travel-Concierge", "Hospital-Docking", "Insurance-Liaison"])
            }
        }

    def run_expansion(self, count=5000, batch_size=500):
        print(f"ğŸš€ å¯åŠ¨æ³¨å¡‘ç¨‹åºï¼šç›®æ ‡å¢åŠ  {count} ä¸ªä¸“å®¶èŠ‚ç‚¹...")
        start_time = time.time()
        
        for i in range(0, count, batch_size):
            batch = [self.generate_clinical_fingerprint(j) for j in range(i, i + batch_size)]
            
            self.collection.upsert(
                ids=[x["id"] for x in batch],
                documents=[x["document"] for x in batch],
                metadatas=[x["metadata"] for x in batch]
            )
            print(f"âœ… å·²æ³¨å…¥: {i + batch_size}/{count}")
            
        print(f"ğŸ æ‰©å®¹å®Œæˆã€‚æ€»è€—æ—¶: {time.time() - start_time:.2f}s")
        print(f"ğŸ“Š å½“å‰åº“ä¸­ä¸“å®¶èµ„äº§æ€»æ•°: {self.collection.count()}")

if __name__ == "__main__":
    engine = AMAHScaleEngine()
    engine.run_expansion(5000)
